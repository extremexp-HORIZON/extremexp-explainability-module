{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from helpers.logger import LoggerHelper, logging\n",
    "from helpers.config import ConfigHelper\n",
    "from helpers import config\n",
    "from helpers import logger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from classes import preprocessing_functions\n",
    "from classes.multiclass_models import NeuralNetwork, ConvolutionalNeuralNetwork, RecurrentNeuralNetwork, LongShortTermMemory\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler,MinMaxScaler\n",
    "from modules.lib import *\n",
    "from modules.optimizer import *\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('workflows_all.json') as f:\n",
    "    json_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('metadata/proxy_data_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-23 10:32:41,467 - classes.preprocessing_functions - INFO - read_data(): Read data from path data_subset\n",
      "2024-10-23 10:32:41,470 - classes.preprocessing_functions - INFO - read_data(): 1 variables are read: ['f3']\n",
      "2024-10-23 10:33:06,945 - classes.preprocessing_functions - INFO - read_data(): Number of files read 648\n"
     ]
    }
   ],
   "source": [
    "indicator_list = [\"f3\"]\n",
    "\n",
    "X, Y, Z = preprocessing_functions.read_data('data_subset', indicator_list)\n",
    "#2 not anomalous\n",
    "#0 electrical\n",
    "#1 mechanical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-23 10:33:06,982 - classes.preprocessing_functions - INFO - add_padding(): Matching the length of the time series adding padding\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X_pad = preprocessing_functions.add_padding(X, indicator_list)\n",
    "\n",
    "Y_encoded = preprocessing_functions.encode_response_variable(Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,z_train,z_test = preprocessing_functions.split_data(X_pad, Y_encoded, Z)\n",
    "\n",
    "n_timestamps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "n_classes = y_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.layers import Masking, Dense, Flatten, Conv1D, MaxPooling1D, SimpleRNN, LSTM\n",
    "from keras import Input\n",
    "\n",
    "def create_model(n_timestamps, n_features, activation_function='relu', units=(50, 50,50), n_classes=2):\n",
    "    model = Sequential()\n",
    "    # Input layer\n",
    "    model.add(Input(shape=(n_timestamps, n_features)))\n",
    "    # Masking layer\n",
    "    model.add(Masking(mask_value=0))\n",
    "    # Fully connected layers\n",
    "    for unit in units:\n",
    "        model.add(Dense(units=unit, activation=activation_function))\n",
    "    # Flatten layer\n",
    "    model.add(Flatten())\n",
    "    # Output layer\n",
    "    model.add(Dense(n_classes, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KerasClassifier(build_fn=create_model, \n",
    "#                         n_timestamps=n_timestamps, \n",
    "#                         n_features=n_features, \n",
    "#                         n_classes=n_classes, \n",
    "#                         verbose=0)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'model__units': [[100, 100, 100],[256, 256, 128],[512,512,512]],     # Example values\n",
    "    'model__activation_function': ['relu','tanh'],       # Example values\n",
    "    'batch_size': (16,32,64) ,                        # Example values\n",
    "    'epochs': (10,50),                       # Example values\n",
    "}\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=2)\n",
    "# grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_models = {}\n",
    "# from itertools import product\n",
    "\n",
    "# combinations = list(product(param_grid['model__units'], \n",
    "#                             param_grid['model__activation_function'], \n",
    "#                             param_grid['batch_size'], \n",
    "#                             param_grid['epochs']))\n",
    "\n",
    "# # Loop through each combination of hyperparameters\n",
    "# for i, (units, activation_function, batch_size, epochs) in enumerate(combinations, 1):\n",
    "#     print(f\"Workflow {i} with units={units}, activation={activation_function}, batch_size={batch_size}, epochs={epochs}\")\n",
    "    \n",
    "# #     # Create and train the model\n",
    "# #     model = create_model(n_timestamps=n_timestamps, n_features=n_features, activation_function=activation_function, units=units, n_classes=3)\n",
    "    \n",
    "# #     model.fit(X_train, y_train, \n",
    "# #               batch_size=batch_size, \n",
    "# #               epochs=epochs)\n",
    "    \n",
    "# #     # Store the trained model in the dictionary with key i (model number)\n",
    "# #     trained_models[i] = model\n",
    "\n",
    "# # # Save the trained models dictionary to a .pkl file\n",
    "# # with open('trained_models.pkl', 'wb') as file:\n",
    "# #     pickle.dump(trained_models, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill as pickle\n",
    "# with open('metadata/proxy_data_models/Ideko_nn_grid_new.pkl', 'wb') as file:\n",
    "#     pickle.dump(grid_result, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "with open('metadata/proxy_data_models/Ideko_nn_grid_new.pkl', 'rb') as f:\n",
    "                            original_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import xai_service_pb2\n",
    "import xai_service_pb2_grpc\n",
    "with grpc.insecure_channel('localhost:50051') as channel:\n",
    "    stub = xai_service_pb2_grpc.ExplanationsStub(channel)\n",
    "    k = stub.GetExplanation(xai_service_pb2.ExplanationsRequest(explanation_type='hyperparameterExplanation',explanation_method='2dpdp',workflows=str(json_data), model='Ideko_model',feature1='epochs',feature2='units'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import xai_service_pb2\n",
    "import xai_service_pb2_grpc\n",
    "with grpc.insecure_channel('localhost:50051') as channel:\n",
    "    stub = xai_service_pb2_grpc.ExplanationsStub(channel)\n",
    "    k = stub.GetExplanation(xai_service_pb2.ExplanationsRequest(explanation_type='hyperparameterExplanation',explanation_method='counterfactuals',model='Ideko_model'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(original_model.best_estimator_.predict(X_test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.argmax(y_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(columns=['Predictions','Labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['Predictions'] = list(preds)\n",
    "predictions['Labels'] = list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy = pd.DataFrame(columns = ['hyperparameters','Label'])\n",
    "for i,params_dict in enumerate(original_model.cv_results_['params']):\n",
    "    # mdl = deepcopy(original_model.estimator)\n",
    "    # mdl.set_params(**params_dict)\n",
    "    # mdl.fit(X_train, y_train)\n",
    "    # prediction = np.argmax(mdl.predict([X_test[3]]),axis=1)\n",
    "    proxy = proxy.append({'hyperparameters' : params_dict},ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy['Label'] = 1\n",
    "\n",
    "# Set the first 6 values to 2\n",
    "proxy.loc[:8, 'Label'] = 2\n",
    "\n",
    "# Set the next 4 values to 1\n",
    "proxy.loc[8:14, 'Label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(proxy['hyperparameters'].iloc[0].keys())\n",
    "\n",
    "# Create new columns for each key\n",
    "for key in keys:\n",
    "    proxy[key] = proxy['hyperparameters'].apply(lambda x: x.get(key, None))\n",
    "\n",
    "# Drop the original \"Hyperparameters\" column\n",
    "proxy_dataset = proxy.drop(columns=['hyperparameters'])\n",
    "proxy_dataset['Label'] = proxy_dataset['Label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = transform_grid(original_model.param_grid)\n",
    "param_space, name = dimensions_aslists(param_grid)\n",
    "space = Space(param_space)\n",
    "\n",
    "plot_dims = []\n",
    "for row in range(space.n_dims):\n",
    "    if space.dimensions[row].is_constant:\n",
    "        continue\n",
    "    plot_dims.append((row, space.dimensions[row]))\n",
    "iscat = [isinstance(dim[1], Categorical) for dim in plot_dims]\n",
    "categorical = [name[i] for i,value in enumerate(iscat) if value == True]\n",
    "proxy_dataset[categorical] = proxy_dataset[categorical].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_transf = ColumnTransformer(transformers=[(\"cat\", OneHotEncoder(), categorical)], remainder=\"passthrough\")\n",
    "\n",
    "proxy_model = Pipeline([\n",
    "    (\"one-hot\", cat_transf),\n",
    "    (\"svm\", SVC(kernel='linear', C=2.0 ,probability=True))\n",
    "])\n",
    "\n",
    "proxy_model = proxy_model.fit(proxy_dataset.drop(columns='Label'), proxy_dataset['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = json.load(open(\"metadata/models.json\"))\n",
    "import joblib\n",
    "# with open(models['Ideko_model']['cfs_surrogate_model'], 'wb') as f:\n",
    "#     joblib.dump(proxy_model, models['Ideko_model']['cfs_surrogate_model'])  \n",
    "# proxy_dataset.to_csv(models['Ideko_model']['cfs_surrogate_dataset'])\n",
    "proxy_dataset = pd.read_csv(models['Ideko_model']['cfs_surrogate_dataset'],index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = pd.DataFrame(data = {'model__units': [params['model__units']], 'model__activation_function': params['model__activation_function'], 'batch_size':params['batch_size'],'epochs':params['epochs']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model__units</th>\n",
       "      <th>model__activation_function</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[100, 100, 100]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model__units model__activation_function  batch_size  epochs\n",
       "0  [100, 100, 100]                       tanh          16      50"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query instance (original outcome : 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>model__activation_function</th>\n",
       "      <th>model__units</th>\n",
       "      <th>BinaryLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td>tanh</td>\n",
       "      <td>[100, 100, 100]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_size  epochs model__activation_function     model__units  BinaryLabel\n",
       "0          16      50                       tanh  [100, 100, 100]            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diverse Counterfactual set (new outcome: 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>model__activation_function</th>\n",
       "      <th>model__units</th>\n",
       "      <th>BinaryLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>tanh</td>\n",
       "      <td>[100, 100, 100]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>tanh</td>\n",
       "      <td>[25, 25, 25]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>relu</td>\n",
       "      <td>[100, 100, 100]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>tanh</td>\n",
       "      <td>[100, 100, 100]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>tanh</td>\n",
       "      <td>[100, 100, 100]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_size  epochs model__activation_function     model__units  BinaryLabel\n",
       "0          16      17                       tanh  [100, 100, 100]            2\n",
       "1          16      16                       tanh     [25, 25, 25]            2\n",
       "2          16      17                       relu  [100, 100, 100]            2\n",
       "3          16      18                       tanh  [100, 100, 100]            2\n",
       "4          16      15                       tanh  [100, 100, 100]            2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "e1.visualize_as_dataframe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xxp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
